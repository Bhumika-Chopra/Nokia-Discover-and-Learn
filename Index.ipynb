{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Jupyter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "class sentence:\n",
    "    def __init__(self, docname, stemwords, originalwords):\n",
    "        self.docname = docname\n",
    "        self.stemwords = stemwords\n",
    "        self.wordfreq = self.sentenceswordfreq()\n",
    "        self.orignalwords = originalwords\n",
    "    \n",
    "    def getdocname(self):\n",
    "        return self.docname\n",
    "    \n",
    "    def getstemwords(self):\n",
    "        return self.stemwords\n",
    "    \n",
    "    def getoriginalwords(self):\n",
    "        return self.originalwords\n",
    "    \n",
    "    def sentenceswordfreq(self):\n",
    "        wordfreq = {}\n",
    "        for word in stemwords:\n",
    "            if word not in wordfreq.keys():\n",
    "                wordfreq[word] = 1\n",
    "            else:\n",
    "                wordfreq[word] = wordfreq[word] + 1\n",
    "        return wordfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "class preprocessing:\n",
    "    def __init__(self, docname, doc):\n",
    "        self. docname = docname\n",
    "        self.doc = doc\n",
    "        \n",
    "    def removestopwords(self):\n",
    "        words = word_tokenize(doc)\n",
    "        stoplist = []\n",
    "        stopwords = set(stopwords.words(\"English\"))\n",
    "        for word in words:\n",
    "            if word not in stopwords:\n",
    "                stoplist.append(word)\n",
    "        return stoplist\n",
    "    \n",
    "    def stemming(sentences):\n",
    "         \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "But a sudden loss of the disputed subsidies could conceivably cause the health care program to implode, leaving millions of people without access to health insurance before Republicans have prepared a replacement.â€œUpon taking office, the Trump administration will evaluate this case and all related aspects of the Affordable Care Act.Insurers that receive the subsidies in exchange for paying    costs such as deductibles and   for eligible consumers could race to drop coverage since they would be losing money.But on spending power and standing, the Trump administration may come under pressure from advocates of presidential authority to fight the House no matter their shared views on health care, since those precedents could have broad repercussions.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "\n",
    "class sentence:\n",
    "    def __init__(self, docname, stemwords, originalwords):\n",
    "        self.docname = docname\n",
    "        self.stemwords = stemwords\n",
    "        self.wordfreq = self.sentenceswordfreq()\n",
    "        self.originalwords = originalwords\n",
    "    \n",
    "    def getdocname(self):\n",
    "        return self.docname\n",
    "    \n",
    "    def getstemwords(self):\n",
    "        return self.stemwords\n",
    "    \n",
    "    def getoriginalwords(self):\n",
    "        return self.originalwords\n",
    "    \n",
    "    def sentenceswordfreq(self):\n",
    "        wordfreq = {}\n",
    "        for word in self.stemwords:\n",
    "            # print(word)\n",
    "            wordfreq.setdefault(word, 1)\n",
    "            wordfreq[word] = wordfreq[word] + 1\n",
    "        return wordfreq\n",
    "    \n",
    "# replace doc by docname \n",
    "def preprocessing(doc):\n",
    "    f = open(doc,'r')\n",
    "    doc1 = f.read()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
    "    lines = tokenizer.tokenize(doc1.strip())\n",
    "    sentences = []\n",
    "    porter = nltk.PorterStemmer()\n",
    "    totalnumwords = 0\n",
    "    for line in lines:\n",
    "        originalwords = line[:]\n",
    "        line = line.strip()\n",
    "        words = word_tokenize(line)\n",
    "        stemmedword = [porter.stem(word) for word in words]\n",
    "        stemmedword = filter(lambda x: x!='.'and x!='`'and x!=','and x!='?'and x!=\"'\" and x!='!' and x!='''\"''' and x!=\"''\" and x!=\"'s\", stemmedword)\n",
    "        l = list(stemmedword)\n",
    "        #[] is False in python\n",
    "        if (l):\n",
    "            s = sentence(doc, l, originalwords)\n",
    "            sentences.append(s)\n",
    "    return sentences\n",
    "\n",
    "#divide by tf by totalnumwords\n",
    "def termfreq(sentences):\n",
    "    tf = {}\n",
    "    for sent in sentences:\n",
    "        wordfreq = sent.sentenceswordfreq()\n",
    "        for word in wordfreq:        \n",
    "            if (tf.get(word,0) == 0):\n",
    "                tf[word] = wordfreq[word]\n",
    "            else:\n",
    "                tf[word] = tf[word] + wordfreq[word]\n",
    "    return tf\n",
    "\n",
    "def inversedocfreq(sentences):\n",
    "    N = len(sentences)\n",
    "    idf = 0\n",
    "    idfs = {}\n",
    "    words = {}\n",
    "    \n",
    "    for sent in sentences:\n",
    "\n",
    "        for word in sent.getstemwords():\n",
    "\n",
    "            if sent.sentenceswordfreq().get(word, 0) != 0:\n",
    "                words[word] = words.get(word, 0)+ 1\n",
    "\n",
    "    for word in words:\n",
    "        n = words[word]\n",
    "\n",
    "        try:\n",
    "            idf = math.log10(float(N)/n)\n",
    "        except ZeroDivisionError:\n",
    "            idf = 0\n",
    "\n",
    "        idfs[word] = idf\n",
    "            \n",
    "    return idfs\n",
    "\n",
    "#return a dictionary with keys as tf-idf values\n",
    "def tf_idf(sentences):\n",
    "    tf_idf_w = defaultdict(list)\n",
    "    tf = termfreq(sentences)\n",
    "    idf = inversedocfreq(sentences)\n",
    "#     print(idf)\n",
    "    for word in tf:\n",
    "        tf_idfs = tf[word]*idf[word]\n",
    "        \n",
    "        if tf_idf_w.get(tf_idfs, None) == None:\n",
    "            tf_idf_w[tf_idfs] = [word]\n",
    "        else:\n",
    "            tf_idf_w[tf_idfs].append(word)\n",
    "    return tf_idf_w\n",
    "\n",
    "def cosinesim(sent1, sent2):\n",
    "    s1 = set(sent1.getstemwords())\n",
    "    s2 = set(sent2.getstemwords())\n",
    "    total = s1.union(s2)\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for word in total:\n",
    "        if(word in s1):\n",
    "            l1.append(1)\n",
    "        else:\n",
    "            l1.append(0)\n",
    "        if(word in s2):\n",
    "            l2.append(1)\n",
    "        else:\n",
    "            l2.append(0)\n",
    "       \n",
    "    num = 0\n",
    "    for i in range(len(l1)):\n",
    "        num += l1[i]*l2[i]\n",
    "    \n",
    "    l1 = [i**2 for i in l1]\n",
    "    l2 = [i**2 for i in l2]\n",
    "    \n",
    "    denom = (sum(l1)*sum(l2))**(0.5)\n",
    "    \n",
    "    try:\n",
    "        return float (num/ denom)\n",
    "    except ZeroDivisionError:\n",
    "        return float(\"-inf\")\n",
    "\n",
    "def bestquery(sentences, tf_idf_w, n):\n",
    "    scores = tf_idf_w.keys()\n",
    "    #descending order of scores\n",
    "    keys = []\n",
    "    for key in scores:\n",
    "        keys.append(key)\n",
    "    #print(keys, 1)\n",
    "    sortedkeys = sorted(keys, reverse = True)\n",
    "    query = []\n",
    "    done = False\n",
    "    for s in sortedkeys:\n",
    "        words = tf_idf_w[s]\n",
    "        for word in words:\n",
    "            if(len(query) < n):\n",
    "                query.append(word)\n",
    "            else:\n",
    "                done = True\n",
    "                break\n",
    "        if(done):\n",
    "            break\n",
    "            \n",
    "    return sentence(\"query\", query, query)\n",
    "\n",
    "def bestscoringsent(sentences, query):\n",
    "    maxval = float(\"-inf\")\n",
    "    bestsent = sentences[0]\n",
    "    for sent in sentences:\n",
    "        similarity = cosinesim(sent,query)\n",
    "        if(maxval < similarity):\n",
    "            maxval = similarity\n",
    "            bestsent = sent\n",
    "    \n",
    "    sentences.remove(bestsent)\n",
    "    return bestsent\n",
    "             \n",
    "    \n",
    "def MMR(Si,Sj,l,query):\n",
    "    Sim1 = cosinesim(Si,query)\n",
    "    Sim2 = float(\"-inf\")\n",
    "    for sent in Sj:\n",
    "        max = cosinesim(Si,sent)\n",
    "        if(max > Sim2):\n",
    "            Sim2 = max\n",
    "    MMRscore = l*Sim1 - (1-l)*Sim2\n",
    "    return MMRscore\n",
    "\n",
    "\n",
    "def selectsent(sentences, bestsent, query, summarylength, l):\n",
    "    summary = [bestsent]\n",
    "    sumlen = len(bestsent.getstemwords())\n",
    "    s = sentences[0]\n",
    "    while(sumlen < summarylength):\n",
    "        MMRval = {}\n",
    "        for sent in sentences:\n",
    "            MMRval[sent] = MMR(sent, summary, l, query)\n",
    "        \n",
    "        maxscore = 0\n",
    "        for k,v in MMRval.items():\n",
    "            if(maxscore < v):\n",
    "                maxscore = v\n",
    "                s = k\n",
    "        summary.append(s)\n",
    "        sentences.remove(s)\n",
    "        sumlen += len(s.getstemwords())\n",
    "    return summary\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     print('hi')\n",
    "\n",
    "    mainpath = os.getcwd() + \"/MMR_DataSet\"\n",
    "    files = os.listdir(mainpath)\n",
    "    sentences = []\n",
    "#     print(files)\n",
    "    for file in files:\n",
    "        if(file == '.ipynb_checkpoints'):\n",
    "            continue\n",
    "        sentences += preprocessing(mainpath + \"/\" + file)\n",
    "    #print(sentences[0].__dict__)\n",
    "    TF_IDF_w = tf_idf(sentences)\n",
    "#    print(TF_IDF_w.items())\n",
    "    query = bestquery(sentences,TF_IDF_w, 10)\n",
    "#   print(query.__dict__)\n",
    "    bestsent = bestscoringsent(sentences, query)\n",
    "#    print(bestsent.__dict__)\n",
    "     #Vary length and lambda\n",
    "    summary = selectsent(sentences, bestsent, query, 100, 0.5)\n",
    "    final_summary = \"\"\n",
    "    for sent in summary:\n",
    "        final_summary = final_summary + sent.getoriginalwords()\n",
    "    print(final_summary)\n",
    "    completename = os.path.join(os.getcwd(), \"MMRSummary.txt\")\n",
    "    f = open(completename, 'w+')\n",
    "    f.write(final_summary)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "def max(b):\n",
    "    max = b[0];\n",
    "    for i in range(len(b)):\n",
    "        if(max<b[i]):\n",
    "            max = b[i]\n",
    "    b.remove(max)\n",
    "    return max;\n",
    "a = [1,2]\n",
    "print(max(a))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dictionary = {\"a\": 1, \"b\": 2, \"c\": 3}\n",
    "\n",
    "max_key = max(a_dictionary, key=a_dictionary.get)\n",
    "print(max_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collection import defaultdict\n",
    "\n",
    "a = defaultdict([])\n",
    "b = defaultdict(list)\n",
    "a[3].append(3)\n",
    "print(a)\n",
    "b[3].append(3)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {1:1, 2:2}\n",
    "keys = []\n",
    "for key in dic.keys():\n",
    "    keys.append(key)\n",
    "    \n",
    "keys = sorted(keys, reverse = True )\n",
    "print(keys)\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This repo contains an introduction to [Jupyter](https://jupyter.org) and [IPython](https://ipython.org).\n",
    "\n",
    "Outline of some basics:\n",
    "\n",
    "* [Notebook Basics](../examples/Notebook/Notebook%20Basics.ipynb)\n",
    "* [IPython - beyond plain python](../examples/IPython%20Kernel/Beyond%20Plain%20Python.ipynb)\n",
    "* [Markdown Cells](../examples/Notebook/Working%20With%20Markdown%20Cells.ipynb)\n",
    "* [Rich Display System](../examples/IPython%20Kernel/Rich%20Output.ipynb)\n",
    "* [Custom Display logic](../examples/IPython%20Kernel/Custom%20Display%20Logic.ipynb)\n",
    "* [Running a Secure Public Notebook Server](../examples/Notebook/Running%20the%20Notebook%20Server.ipynb#Securing-the-notebook-server)\n",
    "* [How Jupyter works](../examples/Notebook/Multiple%20Languages%2C%20Frontends.ipynb) to run code in different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get this tutorial and run it on your laptop:\n",
    "\n",
    "    git clone https://github.com/ipython/ipython-in-depth\n",
    "\n",
    "Install IPython and Jupyter:\n",
    "\n",
    "with [conda](https://www.anaconda.com/download):\n",
    "\n",
    "    conda install ipython jupyter\n",
    "\n",
    "with pip:\n",
    "\n",
    "    # first, always upgrade pip!\n",
    "    pip install --upgrade pip\n",
    "    pip install --upgrade ipython jupyter\n",
    "\n",
    "Start the notebook in the tutorial directory:\n",
    "\n",
    "    cd ipython-in-depth\n",
    "    jupyter notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
